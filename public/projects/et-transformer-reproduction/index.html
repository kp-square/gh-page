<!DOCTYPE html>
<html>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Krishna's Cosmic Portfolio</title>
    <link
      href="https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined" rel="stylesheet" />

    <title>Optimized Transformer implementation in GPU - Krishna Panthi</title>
    <link rel="stylesheet" href="/css/style.css">
  </head>
  <body>
    <div class="cosmic-bg" id="starfield">
    </div>
    <div class="container">
      <header>
    <div class="logo"><img class="logo-img" src="/images/KP.svg" alt="logo">    </div>
    <nav>
        <a href="http://localhost:1313/">Home</a>
        <a href="http://localhost:1313/blog/">Blog</a>
        <a href="http://localhost:1313/projects/">Projects</a>
    </nav>
    
</header>

      <main>
<article class="single">
    <time>December 3, 2024</time>
    
    <div style="text-align:center">


<div>
<h1>Optimized Transformer implementation in GPU</h1>
<h4>Krishna Panthi </h4>
<i>School of Computing, Clemson University<i>
<br>
<i>kpanthi@clemson.edu</i>
</div>
<div class="paper">
<a class="paper-link" href="/pdfs/ET_Parch_Final_Report_Krishna_Panthi.pdf">Paper</a>

</div>


In this research, we attempt to reproduce the work done in the paper "E.T.: Re-Thinking Self-Attention for Transformer Models on GPUs".
Full report is presented in the paper.

</div>


    
    
</article>
</main>
      <footer>
    <p class="footer-text">&copy; 2025 Krishna Panthi</p>
</footer>
    </div>
  </body>
  <script src="/scripts/index.js"></script>
</html>
